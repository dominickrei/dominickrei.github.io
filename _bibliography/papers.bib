---
---

@string{aps = {American Physical Society,}}

@InProceedings{reilly2025egoexo,
  author = {Dominick Reilly and Manish Kumar Govind and Srijan Das},
  booktitle = {arXiv},
  title = {From My View to Yours: Ego-Augmented Learning in Large Vision Language Models for Understanding Exocentric Daily Living Activities},
  abstract = {Large Vision Language Models (LVLMs) have demonstrated impressive capabilities in video understanding, yet their adoption for Activities of Daily Living (ADL) remains limited by their inability to capture fine-grained interactions and spatial relationships. This limitation is particularly evident in ADL tasks, where understanding detailed human-object interaction and human-centric motion is crucial for applications such as elderly monitoring and cognitive assessment. To address this, we aim to leverage the complementary nature of egocentric views to enhance LVLM's understanding of exocentric ADL videos. Consequently, we propose an online ego2exo distillation approach to learn ego-augmented exo representations in LVLMs. While effective, this approach requires paired ego-exo training data, which is impractical to collect for real-world ADL scenarios. Consequently, we develop EgoMimic, a skeleton-guided method that can generate mimicked ego views from exocentric videos. We find that the exo representations of our ego-augmented LVLMs successfully learn to extract ego-perspective cues, demonstrated through comprehensive evaluation on six ADL benchmarks and our proposed EgoPerceptionMCQ benchmark designed specifically to assess egocentric understanding from exocentric videos. Code, models, and data will be open-sourced.},
  year = {2025},
  volume = {},
  number = {},
  pages = {},
  doi = {},
  selected = {true},
  preview = "egoexo.png",
  code = "https://github.com/dominickrei/EgoExo4ADL",
  arxiv = "2501.05711",
}

@InProceedings{reilly2025egoexo,
  author = {Arkaprava Sinha and Dominick Reilly and Francois Bremond and Pu Wang and Srijan Das},
  booktitle = {AAAI},
  title = {SKI Models: Skeleton Induced Vision-Language Embeddings for Understanding Activities of Daily Living},
  abstract = {The introduction of vision-language models like CLIP has enabled the development of foundational video models capable of generalizing to unseen videos and human actions. However, these models are typically trained on web videos, which often fail to capture the challenges present in Activities of Daily Living (ADL) videos. Existing works address ADL-specific challenges, such as similar appearances, subtle motion patterns, and multiple viewpoints, by combining 3D skeletons and RGB videos. However, these approaches are not integrated with language, limiting their ability to generalize to unseen action classes. In this paper, we introduce SKI models, which integrate 3D skeletons into the vision-language embedding space. SKI models leverage a skeleton-language model, SkeletonCLIP, to infuse skeleton information into Vision Language Models (VLMs) and Large Vision Language Models (LVLMs) through collaborative training. Notably, SKI models do not require skeleton data during inference, enhancing their robustness for real-world applications. The effectiveness of SKI models is validated on three popular ADL datasets for zero-shot action recognition and video caption generation tasks.},
  year = {2025},
  volume = {},
  number = {},
  pages = {},
  doi = {},
  selected = {true},
  preview = "ski_models.png",
  code = "https://github.com/thearkaprava/SKI-Models",
  arxiv = "2502.03459",
}

@InProceedings{llavidal2024,
  author = {Dominick Reilly and Rajatsubhra Chakraborty and Arkaprava Sinha and Manish Kumar Govind and Pu Wang and Francois Bremond and Le Xue and Srijan Das},
  booktitle = {arXiv},
  title = {LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living},
  abstract = {Current Large Language Vision Models (LLVMs) trained on web videos perform well in general video understanding but struggle with fine-grained details, complex human-object interactions (HOI), and view-invariant representation learning essential for Activities of Daily Living (ADL). This limitation stems from a lack of specialized ADL video instruction-tuning datasets and insufficient modality integration to capture discriminative action representations. To address this, we propose a semi-automated framework for curating ADL datasets, creating ADL-X, a multiview, multimodal RGBS instruction-tuning dataset. Additionally, we introduce LLAVIDAL, an LLVM integrating videos, 3D skeletons, and HOIs to model ADL's complex spatiotemporal relationships. For training LLAVIDAL a simple joint alignment of all modalities yields suboptimal results; thus, we propose a Multimodal Progressive (MMPro) training strategy, incorporating modalities in stages following a curriculum. We also establish ADL MCQ and video description benchmarks to assess LLVM performance in ADL tasks. Trained on ADL-X, LLAVIDAL achieves state-of-the-art performance across ADL benchmarks. Code and data will be made publicly available.},
  year = {2024},
  volume = {},
  number = {},
  pages = {},
  doi = {},
  selected = {true},
  preview = "llavidal.png",
  code = "https://adl-x.github.io/",
  arxiv = "2406.09390",
}

@InProceedings{ali2024fibottention,
  author = {Ali Khaleghi Rahimian and Manish Kumar Govind and Subhajit Maity and Dominick Reilly and Christian Kümmerle and Srijan Das and Aritra Dutta},
  booktitle = {arXiv},
  title = {Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads},
  abstract = {Visual perception tasks are predominantly solved by Vision Transformer (ViT) architectures, which, despite their effectiveness, encounter a computational bottleneck due to the quadratic complexity of computing self-attention. This inefficiency is largely due to the self-attention heads capturing redundant token interactions, reflecting inherent redundancy within visual data. Many works have aimed to reduce the computational complexity of self-attention in ViTs, leading to the development of efficient and sparse transformer architectures. In this paper, viewing through the efficiency lens, we realized that introducing any sparse self-attention strategy in ViTs can keep the computational overhead low. However, these strategies are sub-optimal as they often fail to capture fine-grained visual details. This observation leads us to propose a general, efficient, sparse architecture, named Fibottention, for approximating self-attention with superlinear complexity that is built upon Fibonacci sequences. The key strategies in Fibottention include: it excludes proximate tokens to reduce redundancy, employs structured sparsity by design to decrease computational demands, and incorporates inception-like diversity across attention heads. This diversity ensures the capture of complementary information through non-overlapping token interactions, optimizing both performance and resource utilization in ViTs for visual representation learning. We embed our Fibottention mechanism into multiple state-of-the-art transformer architectures dedicated to visual tasks. Leveraging only 2-6% of the elements in the self-attention heads, Fibottention in conjunction with ViT and its variants, consistently achieves significant performance boosts compared to standard ViTs in nine datasets across three domains — image classification, video understanding, and robot learning tasks.},
  year = {2024},
  volume = {},
  number = {},
  pages = {},
  doi = {},
  selected = {false},
  preview = "fibottention.png",
  code = "https://github.com/Charlotte-CharMLab/Fibottention",
  arxiv = "2406.19391",
}

@InProceedings{reilly2024pivit,
  author = {Dominick Reilly and Srijan Das},
  booktitle = {CVPR},
  title = {Just Add π! Pose Induced Video Transformers for Understanding Activities of Daily Living},
  abstract = "Video transformers have become the de facto standard for human action recognition, yet their exclusive reliance on the RGB modality still limits their adoption in certain domains. One such domain is Activities of Daily Living (ADL), where RGB alone is not sufficient to distinguish between visually similar actions, or actions observed from multiple viewpoints. To facilitate the adoption of video transformers for ADL, we hypothesize that the augmentation of RGB with human pose information, known for its sensitivity to fine-grained motion and multiple viewpoints, is essential. Consequently, we introduce the first Pose Induced Video Transformer: PI-ViT (or π-ViT), a novel approach that augments the RGB representations learned by video transformers with 2D and 3D pose information. The key elements of π-ViT are two plug-in modules, 2D Skeleton Induction Module and 3D Skeleton Induction Module, that are responsible for inducing 2D and 3D pose information into the RGB representations. These modules operate by performing pose-aware auxiliary tasks, a design choice that allows π-ViT to discard the modules during inference. Notably, π-ViT achieves the state-of-the-art performance on three prominent ADL datasets, encompassing both real-world and large-scale RGB-D datasets, without requiring poses or additional computational overhead at inference.",
  year = {2024},
  volume = {},
  number = {},
  pages = {},
  doi = {},
  selected = {true},
  preview = "pivit.png",
  code = "https://github.com/dominickrei/pi-vit",
  arxiv = "2311.18840",
}

@InProceedings{das2024limited-data-vit,
  author = {Srijan Das and Tanmay Jain and Dominick Reilly and Pranav Balaji and Soumyajit Karmakar and Shyam Marjit and Xiang Li and Abhijit Das and Michael Ryoo},
  booktitle = {WACV},
  title = {Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders},
  abstract = "Vision Transformers (ViTs) have become ubiquitous in computer vision. Despite their success, ViTs lack inductive biases, which can make it difficult to train them with limited data. To address this challenge, prior studies suggest training ViTs with self-supervised learning (SSL) and fine-tuning sequentially. However, we observe that jointly optimizing ViTs for the primary task and a Self-Supervised Auxiliary Task (SSAT) is surprisingly beneficial when the amount of training data is limited. We explore the appropriate SSL tasks that can be optimized alongside the primary task, the training schemes for these tasks, and the data scale at which they can be most effective. Our findings reveal that SSAT is a powerful technique that enables ViTs to leverage the unique characteristics of both the self-supervised and primary tasks, achieving better performance than typical ViTs pre-training with SSL and fine-tuning sequentially. Our experiments, conducted on 10 datasets, demonstrate that SSAT significantly improves ViT performance while reducing carbon footprint. We also confirm the effectiveness of SSAT in the video domain for deepfake detection, showcasing its generalizability.",
  year = {2024},
  volume = {},
  number = {},
  pages = {},
  doi = {},
  selected = {true},
  preview = "smalldatavit.png",
  code = "https://github.com/dominickrei/Limited-data-vits",
  arxiv = "2310.20704",
}

InProceedings{reilly2023poseinpixels,
  author = {Dominick Reilly and Aman Chadha and Srijan Das},
  booktitle = {arXiv},
  title = {Seeing the Pose in the Pixels: Learning Pose-Aware Representations in Vision Transformers},
  abstract = "Human perception of surroundings is often guided by the various poses present within the environment. Many computer vision tasks, such as human action recognition and robot imitation learning, rely on pose-based entities like human skeletons or robotic arms. However, conventional Vision Transformer (ViT) models uniformly process all patches, neglecting valuable pose priors in input videos. We argue that incorporating poses into RGB data is advantageous for learning fine-grained and viewpoint-agnostic representations. Consequently, we introduce two strategies for learning pose-aware representations in ViTs. The first method, called Pose-aware Attention Block (PAAB), is a plug-and-play ViT block that performs localized attention on pose regions within videos. The second method, dubbed Pose-Aware Auxiliary Task (PAAT), presents an auxiliary pose prediction task optimized jointly with the primary ViT task. Although their functionalities differ, both methods succeed in learning pose-aware representations, enhancing performance in multiple diverse downstream tasks. Our experiments, conducted across seven datasets, reveal the efficacy of both pose-aware methods on three video analysis tasks, with PAAT holding a slight edge over PAAB. Both PAAT and PAAB surpass their respective backbone Transformers by up to 9.8% in real-world action recognition and 21.8% in multi-view robotic video alignment.",
  year = {2023},
  volume = {},
  number = {},
  pages = {},
  doi = {},
  selected = {false},
  preview = "poseinpixels.png",
  code = "https://github.com/dominickrei/PoseAwareVT",
  arxiv = "2306.09331",
}

InProceedings{das2023smalldatavit,
  author = {Srijan Das and Tanmay Jain and Dominick Reilly and Soumyajit Karmakar and Shyam Marjit and Xiang Li and Michael Ryoo},
  booktitle = {arXiv},
  title = {From Few to More: Enhancing ViT Performance on Limited Data},
  abstract = "Vision Transformers (ViTs) have become ubiquitous in computer vision. Despite their success, ViTs lack inductive biases, which can make it difficult to train them with limited data. To address this challenge, prior studies suggest training ViTs with self-supervised learning (SSL) and fine-tuning sequentially. However, we observe that jointly optimizing ViTs for the primary task and a Self-Supervised Auxiliary Task (SSAT) is surprisingly beneficial when the amount of training data is limited. We explore the appropriate SSL tasks that can be optimized alongside the primary task, the training schemes for these tasks, and the data scale at which they can be most effective. Our findings reveal that SSAT is a powerful technique that enables ViTs to leverage the unique characteristics of both the self-supervised and primary tasks, achieving better performance than typical ViTs pre-training with SSL and fine-tuning sequentially. Our experiments, conducted on 10 datasets, demonstrate that SSAT significantly improves ViT performance while reducing carbon footprint.",
  year = {2023},
  volume = {},
  number = {},
  pages = {},
  doi = {},
  selected = {true},
  preview = "smalldatavit.png"
}

@InProceedings{saleem2022dpshield,
  author = {Saleem, Muhammad Usama and Reilly, Dominick and Fan, Liyue},
  booktitle = {International Conference on Extending Database Technology (EDBT)},
  title = {DP-Shield: Face Obfuscation with Differential Privacy},
  abstract = "An immense amount of image data is captured and shared nowadays, e.g., social media and surveillance databases.  Such image data may contain sensitive information, such as faces, which can be misused if in the hands of an adversary. Widely used image privacy solutions obfuscate faces, e.g., via pixelization and blurring, before sharing with untrusted parties.  However, they do not provide quantifiable privacy guarantees and are prone to inference attacks. In this demo, we present DP-Shield, an interactive framework for face image obfuscation under the rigorous notion of differential privacy. DP-Shield showcases our recently proposed obfuscation methods, namely DP-Pix and DP-SVD, and also includes two alternative methods for comparison.  The audience will be able to learn about existing DP methods by interacting with them using real-world face image datasets.  Furthermore, DP-Shield integrates widely used image quality measures and practical privacy risk measures (i.e., face recognition) to illustrate the efficacy of our methods.",
  year = {2022},
  volume = {},
  number = {},
  pages = {},
  doi = {},
  selected = {false},
  preview = "DPSHIELD.png",
  website = "http://3.223.148.187/",
  pdf = "https://openproceedings.org/2022/conf/edbt/paper-150.pdf",
}

@InProceedings{reilly2021comparative,
  author={Reilly, Dominick and Fan, Liyue},
  booktitle={IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications (TPS-ISA)}, 
  title={A Comparative Evaluation of Differentially Private Image Obfuscation},
  abstract = "Image data may contain sensitive information, such as face and iris, which can be misused if in the hands of an adversary.  As image data is continuously being collected and shared, it is imperative to ensure the privacy of image data.  Widely used image obfuscation methods apply blurring or pixelization to those sensitive regions.  However, they are prone to inference attacks, and do not provide quantifiable privacy guarantees.   Recently, several obfuscation approaches have been proposed to satisfy the rigorous notion of differential privacy. The goal of this work is to provide a comparative evaluation of those previously proposed approaches in the context of obfuscating face and iris images.  We synthesize existing differentially private obfuscation methods and analyze their privacy guarantees.  Furthermore, we conduct an extensive empirical evaluation regarding practical utility and privacy protection, with real-world face and iris image datasets.  We find that DP-SVD outperforms other methods on several privacy and utility measures. Moreover, we provide an in-depth discussion of our results and point to several considerations when applying those differentially private image obfuscation methods.",
  year={2021},
  volume={},
  number={},
  pages={80-89},
  doi={10.1109/TPSISA52974.2021.00009},
  selected={false},
  preview="comparative.png",
  code = "https://github.com/fan-group/dp-image-obf",
  pdf = "https://ieeexplore.ieee.org/document/9750228"}